# RAG SYSTEM DESIGN - INTERVIEW GUIDE
## 45-Minute System Design Interview

---

## ðŸŽ¯ INTERVIEW STRUCTURE (Follow This Flow!)

### **Minutes 0-5: Clarify Requirements**

**YOU:** "Let me make sure I understand the requirements correctly:"

**Functional Requirements:**
- Users ask questions in natural language
- System retrieves relevant documents
- LLM generates answers based on retrieved context
- Handle text documents initially (PDFs, docs, markdown)

**Non-Functional Requirements:**
- **Scale:** Start with 100 users, plan to scale to 10K+
- **Latency:** Sub-second response time (target <500ms)
- **Accuracy:** High accuracy, minimize hallucinations
- **Cost:** Optimize for cost efficiency as we scale

**Clarifying Questions to Ask:**
- "What types of documents? Just text or also images/tables/videos?"
  - **Start simple:** Text-only, mention multimodal as future work
- "What's the expected query volume?"
  - **Start:** 1K queries/day â†’ **Scale:** 50K queries/day
- "Any compliance requirements (data privacy, audit logs)?"
  - **Phase 3 concern**, acknowledge but don't over-design for MVP

**OUTCOME:** Agree on starting with text-only RAG, plan for scale

---

### **Minutes 5-15: High-Level Architecture (Draw This!)**

**YOU:** "Let me start with the high-level architecture..."

**Draw on whiteboard:**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  User   â”‚
â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
     â”‚
     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   API Gateway   â”‚  (Rate limiting, auth)
â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     â”‚
     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚          Application Layer              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚   Ingestion  â”‚   â”‚  Query Service â”‚  â”‚
â”‚  â”‚   Pipeline   â”‚   â”‚                â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     â”‚                        â”‚
     â–¼                        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Vector DBâ”‚            â”‚   LLM    â”‚
â”‚ (Qdrant) â”‚            â”‚ (GPT-4o) â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Explain (30 seconds per component):**

1. **Ingestion Pipeline:**
   - "Documents are chunked into 500-1000 token pieces"
   - "Each chunk is embedded using OpenAI text-embedding-3"
   - "Embeddings stored in vector database with metadata"

2. **Query Service:**
   - "User query is embedded using same model"
   - "Vector similarity search finds top-k relevant chunks"
   - "Chunks + query sent to LLM for answer generation"

3. **Vector Database:**
   - "Use Qdrant for production - open-source, fast, supports filtering"
   - "HNSW index for fast approximate nearest neighbor search"

4. **LLM:**
   - "GPT-4o for quality, GPT-4o-mini for simple queries (cost optimization)"

**KEY POINT:** "This is the core RAG pattern - Retrieval Augmented Generation"

---

### **Minutes 15-25: Deep Dive (Pick Based on Interviewer Interest)**

**Interviewer will likely ask one of these. Be ready!**

#### **Deep Dive 1: "How do you handle scaling from 100 to 10K users?"**

**YOU:** "Great question. I'd approach this in phases..."

**Phase 1 â†’ Phase 2 scaling:**

1. **Horizontal Scaling:**
   - "Deploy on Kubernetes with HPA (Horizontal Pod Autoscaler)"
   - "Scale from 2 pods â†’ 20 pods based on CPU/memory"

2. **Caching Layer (BIGGEST WIN):**
   - "Add Redis for semantic caching"
   - "Hash query embeddings â†’ cache responses"
   - "**Expected:** 40-60% cache hit rate = 10x cost reduction"

3. **LLM Routing:**
   - "Route simple queries to GPT-4o-mini (10x cheaper)"
   - "Complex queries to GPT-4o"
   - "**Classifier:** Simple heuristic or fine-tuned DistilBERT"

4. **Database Optimization:**
   - "Qdrant cluster: 3 nodes with 2x replication (high availability)"
   - "Vector quantization: 4x storage reduction, minimal accuracy loss"

**Key Metrics to Mention:**
- Latency: 2s â†’ 200ms (10x improvement)
- Cost per query: $0.005 â†’ $0.002 (60% reduction)
- Uptime: 99% â†’ 99.9%

---

#### **Deep Dive 2: "How do you prevent hallucinations?"**

**YOU:** "Hallucinations are a critical concern. I'd implement multiple layers..."

**4-Layer Approach:**

1. **Retrieval Quality:**
   - "Hybrid search: Vector (semantic) + BM25 (keyword)"
   - "Ensures we don't miss exact matches like 'Q3 2024 revenue'"
   - "Re-rank top 20 results â†’ top 5 using Cohere Rerank"

2. **Citation Enforcement:**
   - "Prompt instructs LLM to cite sources: [1], [2]"
   - "Post-process: Verify all citations exist"
   - "Flag uncited claims for review"

3. **Confidence Scoring:**
   - "Ask LLM to rate confidence 1-10"
   - "If < 7, add disclaimer: 'Low confidence answer'"

4. **Similarity Threshold:**
   - "If retrieval score < 0.7, return 'No relevant docs found'"
   - "Better to admit 'I don't know' than hallucinate"

**Example:**
- Query: "What is our Mars colonization plan?"
- No relevant docs found â†’ "I don't have information on this topic"
- NOT: "Sure, here's a hallucinated plan..."

---

#### **Deep Dive 3: "How do you optimize costs?"**

**YOU:** "Cost optimization is critical at scale. Three main strategies..."

**1. Caching (40-60% savings):**
- Semantic cache in Redis
- Hash first 10 dimensions of query embedding
- Handles paraphrased queries

**2. LLM Routing (70% savings):**
- 60% simple queries â†’ GPT-4o-mini ($0.15/1M vs $2.50/1M)
- 30% medium â†’ GPT-4o with prompt caching
- 10% complex â†’ Full GPT-4o

**3. Infrastructure Right-sizing:**
- Kubernetes autoscaling (min 3, max 20 pods)
- Reserved instances for base load (-30%)
- Spot instances for batch jobs (-70%)

**Concrete Example:**
```
Before optimization: $5,000/month
- Caching enabled: -$2,000
- LLM routing: -$1,500
- Infrastructure: -$500
After: $1,000/month (80% reduction!)
```

---

#### **Deep Dive 4: "What if documents have images and tables?"**

**YOU:** "That's where multimodal RAG comes in. I'd extend the architecture..."

**Quick Approach:**

1. **Images:**
   - "Use CLIP to embed images (same model for text + images)"
   - "OCR to extract text from images"
   - "Store in separate image_vectors collection"

2. **Tables:**
   - "Extract with Camelot/Tabula"
   - "Serialize to Markdown for embedding"
   - "LLM generates natural language summary"

3. **Unified Retrieval:**
   - "Search both text and image collections"
   - "Reciprocal Rank Fusion to combine scores"
   - "Use GPT-4V (vision model) for final answer"

**Trade-off:**
- **Cost:** 2-3x higher (multimodal LLMs are expensive)
- **Benefit:** 15-50% better accuracy on visual documents
- **When to use:** Medical, engineering, financial docs (image-heavy)

---

### **Minutes 25-35: Handle Follow-ups**

**Common Follow-up Questions:**

**Q: "How do you handle multi-tenancy?"**

**A:** "Use Qdrant payload filtering"
```python
filter = {
    "must": [
        {"key": "org_id", "match": {"value": "org_123"}},
        {"key": "dept_id", "match": {"any": ["engineering", "shared"]}}
    ]
}
```
- Enforced at query time via JWT claims
- Audit logs track all access
- For large tenants (>1K users), dedicated collection

---

**Q: "What about data freshness? Real-time updates?"**

**A:** "Two approaches depending on latency requirements"

1. **Near real-time (5-10 min lag):**
   - Kafka stream â†’ Qdrant upsert pipeline
   - Incremental updates to vector DB

2. **Hybrid (best of both):**
   - RAG for historical/static data
   - Direct API call for live data (stock prices, dashboards)
   - LLM synthesizes both sources

---

**Q: "How do you measure success?"**

**A:** "Three categories of metrics"

1. **Retrieval Quality:**
   - Recall@k (are relevant docs in top-k?)
   - MRR (Mean Reciprocal Rank)

2. **Generation Quality:**
   - Human eval (thumbs up/down)
   - LLM-as-judge for automated eval

3. **Business Metrics:**
   - User retention
   - Query volume growth
   - Support ticket reduction

**Target:** >85% positive feedback, <200ms latency p95

---

### **Minutes 35-40: Trade-offs & Alternatives**

**Interviewer:** "Why did you choose X over Y?"

**Be ready to discuss:**

**Vector DB: Qdrant vs Pinecone**
- **Qdrant:** Self-hosted, cheaper at scale ($200/mo), payload filtering for multi-tenancy
- **Pinecone:** Managed, faster setup, but 3x more expensive
- **Choice:** Pinecone for MVP, migrate to Qdrant at scale

**LLM: GPT-4o vs Self-hosted Llama**
- **GPT-4o:** Best quality, no ops overhead, but expensive at scale
- **Llama 3 70B:** Free inference (just GPU cost), but need DevOps, lower quality
- **Choice:** Start GPT-4o, consider Llama for 80% of simple queries later

**Chunking: Recursive vs Semantic**
- **Recursive:** Fast, simple, works 80% of time
- **Semantic:** Better boundaries (split on topic changes), 2x slower
- **Choice:** Recursive for MVP, semantic for critical documents

**Key Point:** "No silver bullet - optimize for current scale, plan for future"

---

### **Minutes 40-45: Wrap-up & Extensions**

**YOU:** "To summarize, here's what we covered..."

**Core Architecture:**
- RAG pattern: Embed â†’ Retrieve â†’ Generate
- Qdrant for vectors, GPT-4o for LLM
- Scales to 10K+ users with caching + horizontal scaling

**Key Optimizations:**
- Caching (40-60% cost savings)
- LLM routing (70% savings)
- Hybrid search (better accuracy)

**Future Extensions (if time permits):**

1. **Agentic RAG:**
   - LLM decides when to retrieve vs use memory
   - Multi-hop reasoning for complex queries
   - Self-reflection to improve answers

2. **Graph RAG:**
   - Build knowledge graph from documents
   - Better at multi-hop questions ("Who reports to the CEO of company X?")

3. **Fine-tuning:**
   - Fine-tune small model on domain data
   - 90% cost reduction, similar quality for simple queries

**Final Question to Interviewer:** "Any specific area you'd like me to deep dive further?"

---

## ðŸŽ“ KEY TALKING POINTS TO MEMORIZE

### **Numbers to Remember:**

| Metric | MVP | Growth | Enterprise |
|--------|-----|--------|------------|
| **Users** | 100 | 10K | 100K+ |
| **Cost/month** | $150 | $3K | $20K |
| **Cost/query** | $0.005 | $0.002 | $0.0013 |
| **Latency p95** | 2s | 200ms | 100ms |
| **Cache hit rate** | 0% | 50% | 70% |

### **Technology Choices:**

| Component | Choice | Why? |
|-----------|--------|------|
| **Vector DB** | Qdrant | Open-source, fast, payload filtering |
| **Embedding** | OpenAI text-embed-3 | Best quality/cost, 1536 dims |
| **LLM** | GPT-4o / GPT-4o-mini | Routing based on complexity |
| **Cache** | Redis | Fast, semantic caching support |
| **Compute** | Kubernetes | Auto-scaling, HA |

### **Optimization Techniques:**

1. **Caching** â†’ 40-60% savings
2. **LLM Routing** â†’ 70% savings
3. **Hybrid Search** â†’ +15% accuracy
4. **Prompt Caching** â†’ 50% input cost savings
5. **Vector Quantization** â†’ 4x storage reduction

---

## ðŸ’¡ INTERVIEW TIPS

### **DO:**
- âœ… Start simple (text-only RAG), add complexity when asked
- âœ… Draw diagrams (visual learners love this)
- âœ… Mention trade-offs for every decision
- âœ… Use concrete numbers (latency, cost, accuracy)
- âœ… Ask clarifying questions upfront
- âœ… Relate to real-world experience if you have it

### **DON'T:**
- âŒ Jump into implementation details too early
- âŒ Over-engineer for edge cases
- âŒ Ignore cost optimization (huge red flag)
- âŒ Forget about monitoring/observability
- âŒ Say "I don't know" without proposing an approach

### **If Stuck:**
- "That's a great question. Let me think through the trade-offs..."
- "I'd approach this in phases - MVP first, then optimize"
- "I haven't implemented this exact scenario, but here's how I'd think about it..."

### **Power Moves:**
- Mention you read the latest research (RAPTOR, HyDE, etc.)
- Discuss A/B testing prompt strategies
- Talk about LLM observability (LangSmith, LangFuse)
- Bring up real cost numbers from experience

---

## ðŸŽ¯ SAMPLE INTERVIEW TRANSCRIPT

**Interviewer:** "Design a RAG system for our company's internal knowledge base."

**You:** "Great! Let me start by clarifying requirements. What scale are we targeting initially?"

**Interviewer:** "Start with 100 employees, but plan for 10K."

**You:** "Got it. Are we dealing with just text documents or also images, tables, videos?"

**Interviewer:** "Primarily text documents - Confluence, Google Docs, PDFs."

**You:** "Perfect. I'll design a text-based RAG system. Let me start with the high-level architecture..."

*[Draw diagram on whiteboard]*

**You:** "At a high level, we have two main flows: ingestion and query. For ingestion, documents are chunked, embedded, and stored. For queries, we retrieve relevant chunks and generate answers. Let me walk through each component..."

*[Explain each component in 30 seconds]*

**Interviewer:** "How do you handle scaling to 10K users?"

**You:** "Great question. I'd approach this in phases. First, horizontal scaling with Kubernetes. Second - and this is the big one - add a Redis caching layer for semantic caching. This alone gives us 40-60% cost reduction. Third, implement LLM routing..."

*[Continue with scaling discussion]*

**Interviewer:** "What about hallucinations?"

**You:** "Critical concern. I'd implement a 4-layer approach: hybrid search for better retrieval, citation enforcement, confidence scoring, and similarity thresholds..."

*[Explain each layer]*

**Interviewer:** "Sounds good. Let's talk about costs. How would you optimize?"

**You:** "Cost optimization is make-or-break at scale. Three main strategies: caching, LLM routing, and infrastructure rightsizing. Here's a concrete example..."

*[Show before/after numbers]*

**Interviewer:** "Great! Any questions for me?"

**You:** "Yes - what types of queries do you expect to be most common? That would help me optimize the design further."

---

**Total Time: 45 minutes**
**Outcome: Strong hire signal âœ…**
